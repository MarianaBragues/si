{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation 9 and 10\n",
    "\n",
    "#### Exercise 12: Dropout layer\n",
    "A dropout layer in NNs is a regularization technique where a random set of neurons is temporarily ignored (dropped out) during training, helping prevent overfitting by promoting robustness and generalization in the model.\n",
    "\n",
    "\n",
    "12.1) Add a new layer named Dropout to the layers module; Take into consideration the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"layers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.2) Test the layer with a random input and check if the output shows the desired behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"scripts\", sub-pasta \"Tests\", notebook \"test_droupoutlayer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13: TanhActivation and SoftmaxActivation classes\n",
    "The tanh activation layer in NNs applies the hyperbolic tangent function to the output of neurons, squashing the values to the range of -1 to 1. \n",
    "The softmax activation layer in NNs transforms the raw output scores into a probability distribution (that sums to 1), making it suitable for multi-class classification problems.\n",
    "\n",
    "\n",
    "13.1) Add a new activation class named TanhActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"activation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.2) Add a new activation class named SoftmaxActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"layers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14: CategoricalCrossEntropy Class\n",
    "The categorical cross-entropy loss function in neural NNs is applied to multi-class classification problems. It measures the\n",
    "dissimilarity between predicted class probabilities and true one-hot encoded class labels;\n",
    "\n",
    "14.1) Add a new loss function named CategoricalCrossEntropy to the losses module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"losses.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15: Adam class\n",
    "\n",
    "Adam can be looked at as a combination of RMSprop and SGD with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.\n",
    "\n",
    "15.1) Add a new optimizer named Adam to the optimizers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"optimizers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16:\n",
    "\n",
    "16.1) Build , train and evaluate a neural network\n",
    "• Build, train and evaluate a NN based on the following instructions:\n",
    "- The training dataset has 32 features\n",
    "- The task is binary classification\n",
    "- Use the SGD optimizer\n",
    "- Use the BinaryCrossEntropy loss\n",
    "- Use the accuracy metric\n",
    "- The model should contain:\n",
    "    • Dense layer 1\n",
    "    • ReLU activation layer 1\n",
    "    • Dense layer 2\n",
    "    • ReLU activation layer 2\n",
    "    • Output Dense layer\n",
    "    • Sigmoid activation layer\n",
    "- The dense layers should reduce the number of units to half except the last one\n",
    "- Train the NN for 100 epochs , with batch size of 16 with a learning rate of 0.01.\n",
    "- Test the model on na independent test set ( you can create a random train and test set using numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.neural_networks.activation import ActivationLayer, ReLUActivation, SigmoidActivation\n",
    "from si.neural_networks.layers import Layer, DenseLayer\n",
    "from si.neural_networks.optimizers import Optimizer, SGD\n",
    "from si.neural_networks.losses import LossFunction, MeanSquaredError, BinaryCrossEntropy\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.neural_networks.neural_network import NeuralNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random training and test datasets\n",
    "train_data = np.random.rand(1000, 32)\n",
    "train_labels = np.random.randint(0, 2, (1000,))\n",
    "test_data = np.random.rand(200, 32)\n",
    "test_labels = np.random.randint(0, 2, (200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "model = NeuralNetwork([\n",
    "    DenseLayer(32, input_shape=(32,)),\n",
    "    ReLUActivation(),\n",
    "    DenseLayer(16),\n",
    "    ReLUActivation(),\n",
    "    DenseLayer(8),\n",
    "    ReLUActivation(),\n",
    "    DenseLayer(1),\n",
    "    SigmoidActivation()], learning_rate=0.01, batch_size=16, optimizer=SGD(learning_rate=0.01), loss=BinaryCrossEntropy(), metric=accuracy, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'batch_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mariana\\si\\scripts\\evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mariana/si/scripts/evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mariana/si/scripts/evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m NeuralNetwork(epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, optimizer\u001b[39m=\u001b[39mSGD(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m), loss\u001b[39m=\u001b[39mBinaryCrossEntropy(), metric\u001b[39m=\u001b[39maccuracy, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mariana/si/scripts/evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_data)\n",
      "File \u001b[1;32mc:\\Users\\Mariana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\si-0.0.1-py3.10.egg\\si\\neural_networks\\neural_network.py:63\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, dataset: Dataset) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mNeuralNetwork\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     62\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m---> 63\u001b[0m         \u001b[39mfor\u001b[39;00m batch_data \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39;49mbatch_iterator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n\u001b[0;32m     64\u001b[0m             inputs, targets \u001b[39m=\u001b[39m batch_data\n\u001b[0;32m     65\u001b[0m             \u001b[39m# Forward propagation\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'batch_iterator'"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = NeuralNetwork(epochs=100, batch_size=16, optimizer=SGD(learning_rate=0.01), loss=BinaryCrossEntropy(), metric=accuracy, verbose=True, learning_rate=0.01)\n",
    "model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {accuracy[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
