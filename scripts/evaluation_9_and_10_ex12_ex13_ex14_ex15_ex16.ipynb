{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation 9 and 10\n",
    "\n",
    "#### Exercise 12: Dropout layer\n",
    "A dropout layer in NNs is a regularization technique where a random set of neurons is temporarily ignored (dropped out) during training, helping prevent overfitting by promoting robustness and generalization in the model.\n",
    "\n",
    "\n",
    "12.1) Add a new layer named Dropout to the layers module; Take into consideration the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"layers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.2) Test the layer with a random input and check if the output shows the desired behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"scripts\", sub-pasta \"Tests\", notebook \"test_droupoutlayer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13: TanhActivation and SoftmaxActivation classes\n",
    "The tanh activation layer in NNs applies the hyperbolic tangent function to the output of neurons, squashing the values to the range of -1 to 1. \n",
    "The softmax activation layer in NNs transforms the raw output scores into a probability distribution (that sums to 1), making it suitable for multi-class classification problems.\n",
    "\n",
    "\n",
    "13.1) Add a new activation class named TanhActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"activation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.2) Add a new activation class named SoftmaxActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"layers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14: CategoricalCrossEntropy Class\n",
    "The categorical cross-entropy loss function in neural NNs is applied to multi-class classification problems. It measures the\n",
    "dissimilarity between predicted class probabilities and true one-hot encoded class labels;\n",
    "\n",
    "14.1) Add a new loss function named CategoricalCrossEntropy to the losses module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"losses.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15: Adam class\n",
    "\n",
    "Adam can be looked at as a combination of RMSprop and SGD with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.\n",
    "\n",
    "15.1) Add a new optimizer named Adam to the optimizers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"optimizers.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
