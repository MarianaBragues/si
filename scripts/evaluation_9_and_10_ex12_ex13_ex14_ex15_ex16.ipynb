{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation 9 and 10\n",
    "\n",
    "#### Exercise 12: Dropout layer\n",
    "A dropout layer in NNs is a regularization technique where a random set of neurons is temporarily ignored (dropped out) during training, helping prevent overfitting by promoting robustness and generalization in the model.\n",
    "\n",
    "\n",
    "12.1) Add a new layer named Dropout to the layers module; Take into consideration the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"layers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.2) Test the layer with a random input and check if the output shows the desired behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"scripts\", sub-pasta \"Tests\", notebook \"test_droupoutlayer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13: TanhActivation and SoftmaxActivation classes\n",
    "The tanh activation layer in NNs applies the hyperbolic tangent function to the output of neurons, squashing the values to the range of -1 to 1. \n",
    "The softmax activation layer in NNs transforms the raw output scores into a probability distribution (that sums to 1), making it suitable for multi-class classification problems.\n",
    "\n",
    "\n",
    "13.1) Add a new activation class named TanhActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"activation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.2) Add a new activation class named SoftmaxActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"activation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14: CategoricalCrossEntropy Class\n",
    "The categorical cross-entropy loss function in neural NNs is applied to multi-class classification problems. It measures the\n",
    "dissimilarity between predicted class probabilities and true one-hot encoded class labels;\n",
    "\n",
    "14.1) Add a new loss function named CategoricalCrossEntropy to the losses module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"losses.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15: Adam class\n",
    "\n",
    "Adam can be looked at as a combination of RMSprop and SGD with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.\n",
    "\n",
    "15.1) Add a new optimizer named Adam to the optimizers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"optimizers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16:\n",
    "\n",
    "16.1) Build , train and evaluate a neural network\n",
    "• Build, train and evaluate a NN based on the following instructions:\n",
    "- The training dataset has 32 features\n",
    "- The task is binary classification\n",
    "- Use the SGD optimizer\n",
    "- Use the BinaryCrossEntropy loss\n",
    "- Use the accuracy metric\n",
    "- The model should contain:\n",
    "    • Dense layer 1\n",
    "    • ReLU activation layer 1\n",
    "    • Dense layer 2\n",
    "    • ReLU activation layer 2\n",
    "    • Output Dense layer\n",
    "    • Sigmoid activation layer\n",
    "- The dense layers should reduce the number of units to half except the last one\n",
    "- Train the NN for 100 epochs , with batch size of 16 with a learning rate of 0.01.\n",
    "- Test the model on na independent test set ( you can create a random train and test set using numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si.neural_networks.neural_network import NeuralNetwork\n",
    "from si.neural_networks.layers import DenseLayer\n",
    "from si.neural_networks.activation import ReLUActivation, SigmoidActivation\n",
    "from si.neural_networks.losses import BinaryCrossEntropy\n",
    "from si.neural_networks.optimizers import SGD\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.data.dataset import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<si.neural_networks.neural_network.NeuralNetwork at 0x25d9163d120>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the neural network\n",
    "model = NeuralNetwork(epochs=100, batch_size=16, optimizer=SGD, learning_rate=0.01, verbose=True,\n",
    "                        loss=BinaryCrossEntropy, metric=accuracy)\n",
    "model.add(DenseLayer(16, input_shape=(32,)))  # Camada densa 1\n",
    "model.add(ReLUActivation())                  # Ativação ReLU 1\n",
    "model.add(DenseLayer(8))                     # Camada densa 2\n",
    "model.add(ReLUActivation())                  # Ativação ReLU 2\n",
    "model.add(DenseLayer(1))                     # Camada de saída\n",
    "model.add(SigmoidActivation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "train_data = np.random.randn(1000, 32)       \n",
    "train_labels = np.random.randint(0, 2, (1000, 1))  \n",
    "\n",
    "test_data = np.random.randn(200, 32)         \n",
    "test_labels = np.random.randint(0, 2, (200, 1))  \n",
    "\n",
    "train_dataset = Dataset(train_data, train_labels)\n",
    "test_dataset = Dataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - loss: 697.8041 - accuracy: 0.0000\n",
      "Epoch 2/100 - loss: 687.6782 - accuracy: 0.0000\n",
      "Epoch 3/100 - loss: 684.6885 - accuracy: 0.0000\n",
      "Epoch 4/100 - loss: 680.8412 - accuracy: 0.0000\n",
      "Epoch 5/100 - loss: 676.3703 - accuracy: 0.0000\n",
      "Epoch 6/100 - loss: 667.5045 - accuracy: 0.0000\n",
      "Epoch 7/100 - loss: 654.7056 - accuracy: 0.0000\n",
      "Epoch 8/100 - loss: 642.7708 - accuracy: 0.0000\n",
      "Epoch 9/100 - loss: 623.6391 - accuracy: 0.0000\n",
      "Epoch 10/100 - loss: 609.2146 - accuracy: 0.0000\n",
      "Epoch 11/100 - loss: 587.9720 - accuracy: 0.0000\n",
      "Epoch 12/100 - loss: 574.4535 - accuracy: 0.0000\n",
      "Epoch 13/100 - loss: 553.6790 - accuracy: 0.0000\n",
      "Epoch 14/100 - loss: 529.1116 - accuracy: 0.0000\n",
      "Epoch 15/100 - loss: 517.3531 - accuracy: 0.0000\n",
      "Epoch 16/100 - loss: 505.4561 - accuracy: 0.0000\n",
      "Epoch 17/100 - loss: 479.9601 - accuracy: 0.0000\n",
      "Epoch 18/100 - loss: 467.9071 - accuracy: 0.0000\n",
      "Epoch 19/100 - loss: 440.8688 - accuracy: 0.0000\n",
      "Epoch 20/100 - loss: 441.0558 - accuracy: 0.0000\n",
      "Epoch 21/100 - loss: 418.7914 - accuracy: 0.0000\n",
      "Epoch 22/100 - loss: 412.9553 - accuracy: 0.0000\n",
      "Epoch 23/100 - loss: 393.6537 - accuracy: 0.0000\n",
      "Epoch 24/100 - loss: 388.8200 - accuracy: 0.0000\n",
      "Epoch 25/100 - loss: 379.4591 - accuracy: 0.0000\n",
      "Epoch 26/100 - loss: 371.9855 - accuracy: 0.0000\n",
      "Epoch 27/100 - loss: 361.3847 - accuracy: 0.0000\n",
      "Epoch 28/100 - loss: 328.2565 - accuracy: 0.0000\n",
      "Epoch 29/100 - loss: 320.6657 - accuracy: 0.0000\n",
      "Epoch 30/100 - loss: 288.7560 - accuracy: 0.0000\n",
      "Epoch 31/100 - loss: 288.5054 - accuracy: 0.0000\n",
      "Epoch 32/100 - loss: 286.5908 - accuracy: 0.0000\n",
      "Epoch 33/100 - loss: 292.8436 - accuracy: 0.0000\n",
      "Epoch 34/100 - loss: 297.9644 - accuracy: 0.0000\n",
      "Epoch 35/100 - loss: 264.0557 - accuracy: 0.0000\n",
      "Epoch 36/100 - loss: 296.2232 - accuracy: 0.0000\n",
      "Epoch 37/100 - loss: 280.4975 - accuracy: 0.0000\n",
      "Epoch 38/100 - loss: 291.2335 - accuracy: 0.0000\n",
      "Epoch 39/100 - loss: 261.4410 - accuracy: 0.0000\n",
      "Epoch 40/100 - loss: 268.7693 - accuracy: 0.0000\n",
      "Epoch 41/100 - loss: 225.2209 - accuracy: 0.0000\n",
      "Epoch 42/100 - loss: 217.0735 - accuracy: 0.0000\n",
      "Epoch 43/100 - loss: 264.3713 - accuracy: 0.0000\n",
      "Epoch 44/100 - loss: 263.9684 - accuracy: 0.0000\n",
      "Epoch 45/100 - loss: 220.2444 - accuracy: 0.0000\n",
      "Epoch 46/100 - loss: 220.5166 - accuracy: 0.0000\n",
      "Epoch 47/100 - loss: 231.9778 - accuracy: 0.0000\n",
      "Epoch 48/100 - loss: 240.1711 - accuracy: 0.0000\n",
      "Epoch 49/100 - loss: 215.5583 - accuracy: 0.0000\n",
      "Epoch 50/100 - loss: 212.3401 - accuracy: 0.0000\n",
      "Epoch 51/100 - loss: 253.8338 - accuracy: 0.0000\n",
      "Epoch 52/100 - loss: 267.1092 - accuracy: 0.0000\n",
      "Epoch 53/100 - loss: 263.3509 - accuracy: 0.0000\n",
      "Epoch 54/100 - loss: 227.7741 - accuracy: 0.0000\n",
      "Epoch 55/100 - loss: 200.9342 - accuracy: 0.0000\n",
      "Epoch 56/100 - loss: 225.6034 - accuracy: 0.0000\n",
      "Epoch 57/100 - loss: 195.5979 - accuracy: 0.0000\n",
      "Epoch 58/100 - loss: 227.6233 - accuracy: 0.0000\n",
      "Epoch 59/100 - loss: 242.9317 - accuracy: 0.0000\n",
      "Epoch 60/100 - loss: 199.9892 - accuracy: 0.0000\n",
      "Epoch 61/100 - loss: 180.1909 - accuracy: 0.0000\n",
      "Epoch 62/100 - loss: 142.6085 - accuracy: 0.0000\n",
      "Epoch 63/100 - loss: 120.0585 - accuracy: 0.0000\n",
      "Epoch 64/100 - loss: 133.3120 - accuracy: 0.0000\n",
      "Epoch 65/100 - loss: 169.6952 - accuracy: 0.0000\n",
      "Epoch 66/100 - loss: 239.6529 - accuracy: 0.0000\n",
      "Epoch 67/100 - loss: 224.3495 - accuracy: 0.0000\n",
      "Epoch 68/100 - loss: 204.7783 - accuracy: 0.0000\n",
      "Epoch 69/100 - loss: 164.5803 - accuracy: 0.0000\n",
      "Epoch 70/100 - loss: 128.7600 - accuracy: 0.0000\n",
      "Epoch 71/100 - loss: 109.9343 - accuracy: 0.0000\n",
      "Epoch 72/100 - loss: 125.4347 - accuracy: 0.0000\n",
      "Epoch 73/100 - loss: 95.9998 - accuracy: 0.0000\n",
      "Epoch 74/100 - loss: 96.8764 - accuracy: 0.0000\n",
      "Epoch 75/100 - loss: 88.5320 - accuracy: 0.0000\n",
      "Epoch 76/100 - loss: 87.3143 - accuracy: 0.0000\n",
      "Epoch 77/100 - loss: 105.7011 - accuracy: 0.0000\n",
      "Epoch 78/100 - loss: 186.0486 - accuracy: 0.0000\n",
      "Epoch 79/100 - loss: 265.9435 - accuracy: 0.0000\n",
      "Epoch 80/100 - loss: 308.7702 - accuracy: 0.0000\n",
      "Epoch 81/100 - loss: 270.4479 - accuracy: 0.0000\n",
      "Epoch 82/100 - loss: 213.8392 - accuracy: 0.0000\n",
      "Epoch 83/100 - loss: 157.2983 - accuracy: 0.0000\n",
      "Epoch 84/100 - loss: 119.8847 - accuracy: 0.0000\n",
      "Epoch 85/100 - loss: 130.2683 - accuracy: 0.0000\n",
      "Epoch 86/100 - loss: 135.5501 - accuracy: 0.0010\n",
      "Epoch 87/100 - loss: 148.8733 - accuracy: 0.0010\n",
      "Epoch 88/100 - loss: 133.0061 - accuracy: 0.0010\n",
      "Epoch 89/100 - loss: 117.7018 - accuracy: 0.0020\n",
      "Epoch 90/100 - loss: 84.1393 - accuracy: 0.0010\n",
      "Epoch 91/100 - loss: 62.4049 - accuracy: 0.0010\n",
      "Epoch 92/100 - loss: 52.7727 - accuracy: 0.0020\n",
      "Epoch 93/100 - loss: 50.3628 - accuracy: 0.0010\n",
      "Epoch 94/100 - loss: 47.5291 - accuracy: 0.0020\n",
      "Epoch 95/100 - loss: 45.2777 - accuracy: 0.0020\n",
      "Epoch 96/100 - loss: 43.9818 - accuracy: 0.0020\n",
      "Epoch 97/100 - loss: 45.1709 - accuracy: 0.0040\n",
      "Epoch 98/100 - loss: 44.9254 - accuracy: 0.0050\n",
      "Epoch 99/100 - loss: 44.1394 - accuracy: 0.0050\n",
      "Epoch 100/100 - loss: 43.6516 - accuracy: 0.0050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<si.neural_networks.neural_network.NeuralNetwork at 0x25d9163d120>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = model.score(test_dataset)\n",
    "print(f'Test accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
