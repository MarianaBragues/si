{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation 9 and 10\n",
    "\n",
    "#### Exercise 12: Dropout layer\n",
    "A dropout layer in NNs is a regularization technique where a random set of neurons is temporarily ignored (dropped out) during training, helping prevent overfitting by promoting robustness and generalization in the model.\n",
    "\n",
    "\n",
    "12.1) Add a new layer named Dropout to the layers module; Take into consideration the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"layers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.2) Test the layer with a random input and check if the output shows the desired behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"scripts\", sub-pasta \"Tests\", notebook \"test_droupoutlayer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 13: TanhActivation and SoftmaxActivation classes\n",
    "The tanh activation layer in NNs applies the hyperbolic tangent function to the output of neurons, squashing the values to the range of -1 to 1. \n",
    "The softmax activation layer in NNs transforms the raw output scores into a probability distribution (that sums to 1), making it suitable for multi-class classification problems.\n",
    "\n",
    "\n",
    "13.1) Add a new activation class named TanhActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"activation.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.2) Add a new activation class named SoftmaxActivation to the \"activation\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"layers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 14: CategoricalCrossEntropy Class\n",
    "The categorical cross-entropy loss function in neural NNs is applied to multi-class classification problems. It measures the\n",
    "dissimilarity between predicted class probabilities and true one-hot encoded class labels;\n",
    "\n",
    "14.1) Add a new loss function named CategoricalCrossEntropy to the losses module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"losses.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 15: Adam class\n",
    "\n",
    "Adam can be looked at as a combination of RMSprop and SGD with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.\n",
    "\n",
    "15.1) Add a new optimizer named Adam to the optimizers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta \"si\", sub-pasta \"neural_networks\", ficheiro \"optimizers.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 16:\n",
    "\n",
    "16.1) Build , train and evaluate a neural network\n",
    "• Build, train and evaluate a NN based on the following instructions:\n",
    "- The training dataset has 32 features\n",
    "- The task is binary classification\n",
    "- Use the SGD optimizer\n",
    "- Use the BinaryCrossEntropy loss\n",
    "- Use the accuracy metric\n",
    "- The model should contain:\n",
    "    • Dense layer 1\n",
    "    • ReLU activation layer 1\n",
    "    • Dense layer 2\n",
    "    • ReLU activation layer 2\n",
    "    • Output Dense layer\n",
    "    • Sigmoid activation layer\n",
    "- The dense layers should reduce the number of units to half except the last one\n",
    "- Train the NN for 100 epochs , with batch size of 16 with a learning rate of 0.01.\n",
    "- Test the model on na independent test set ( you can create a random train and test set using numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Module 'numpy.core' has no attribute 'numerictypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mariana\\si\\scripts\\evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mariana/si/scripts/evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneural_networks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivation\u001b[39;00m \u001b[39mimport\u001b[39;00m ActivationLayer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mariana/si/scripts/evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneural_networks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Layer, DenseLayer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mariana/si/scripts/evaluation_9_and_10_ex12_ex13_ex14_ex15_ex16.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneural_networks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Optimizer, SGD\n",
      "File \u001b[1;32mc:\\Users\\Mariana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\si-0.0.1-py3.10.egg\\si\\neural_networks\\activation.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m abstractmethod\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union\n\u001b[1;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneural_networks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Layer\n\u001b[0;32m      9\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mActivationLayer\u001b[39;00m(Layer):\n",
      "File \u001b[1;32mc:\\Users\\Mariana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\__init__.py:191\u001b[0m\n\u001b[0;32m    184\u001b[0m __deprecated_attrs__\u001b[39m.\u001b[39mupdate({\n\u001b[0;32m    185\u001b[0m     n: (\u001b[39mgetattr\u001b[39m(_builtins, n), _msg\u001b[39m.\u001b[39mformat(n\u001b[39m=\u001b[39mn, extended_msg\u001b[39m=\u001b[39mextended_msg))\n\u001b[0;32m    186\u001b[0m     \u001b[39mfor\u001b[39;00m n, extended_msg \u001b[39min\u001b[39;00m _type_info\n\u001b[0;32m    187\u001b[0m })\n\u001b[0;32m    189\u001b[0m \u001b[39m# Numpy 1.20.0, 2020-10-19\u001b[39;00m\n\u001b[0;32m    190\u001b[0m __deprecated_attrs__[\u001b[39m\"\u001b[39m\u001b[39mtypeDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 191\u001b[0m     core\u001b[39m.\u001b[39;49mnumerictypes\u001b[39m.\u001b[39mtypeDict,\n\u001b[0;32m    192\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`np.typeDict` is a deprecated alias for `np.sctypeDict`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m )\n\u001b[0;32m    195\u001b[0m \u001b[39m# NumPy 1.22, 2021-10-20\u001b[39;00m\n\u001b[0;32m    196\u001b[0m __deprecated_attrs__[\u001b[39m\"\u001b[39m\u001b[39mMachAr\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[0;32m    197\u001b[0m     core\u001b[39m.\u001b[39m_machar\u001b[39m.\u001b[39mMachAr,\n\u001b[0;32m    198\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`np.MachAr` is deprecated (NumPy 1.22).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mariana\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\__init__.py:161\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    156\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    157\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe `np.core.machar` module is deprecated (NumPy 1.22)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    158\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m _machar\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Module 'numpy.core' has no attribute 'numerictypes'"
     ]
    }
   ],
   "source": [
    "from si.neural_networks.activation import ActivationLayer\n",
    "from si.neural_networks.layers import Layer, DenseLayer\n",
    "from si.neural_networks.optimizers import Optimizer, SGD\n",
    "from si.neural_networks.losses import LossFunction, MeanSquaredError, BinaryCrossEntropy\n",
    "from si.metrics.accuracy import accuracy\n",
    "from si.neural_networks.neural_network import NeuralNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random training and test datasets\n",
    "train_data = np.random.rand(1000, 32)\n",
    "train_labels = np.random.randint(0, 2, (1000,))\n",
    "test_data = np.random.rand(200, 32)\n",
    "test_labels = np.random.randint(0, 2, (200,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "model = NeuralNetwork([\n",
    "    DenseLayer(16, input_shape=(32,), activation='relu'),\n",
    "    DenseLayer(8, activation='relu'),\n",
    "    DenseLayer(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=SGD(learning_rate=0.01),\n",
    "              loss=BinaryCrossEntropy(),\n",
    "              metrics=[accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_data, train_labels, epochs=100, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {accuracy[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
